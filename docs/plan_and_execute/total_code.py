"""
Plan and Execute ì‹¤í–‰ ì˜ˆì œ
- ì°¸ê³  : https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/plan-and-execute/plan-and-execute.ipynb
"""

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


# íˆ´ ì •ì˜
from langchain_community.tools.tavily_search import TavilySearchResults
tools = [TavilySearchResults(max_results=3)]

# llm ì •ì˜
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o")

# agent ì •ì˜
from langgraph.prebuilt import create_react_agent
agent_executor = create_react_agent(llm, tools, prompt="You are a helpful assistant that can answer questions and help with tasks.")

# ë‹¨ì¼ì‹¤í–‰
# result = agent_executor.invoke({"messages": [("user", "ë¶€ì‚° ê´€ê´‘ì§€ 3ê°œ ì†Œê°œí•´ì£¼ì„¸ìš”.")]})
# print(result)

# State ì •ì˜
import operator
from typing import Annotated, List, Tuple
from typing_extensions import TypedDict

class PlanExecute(TypedDict):
    input: str
    plan: List[str]
    past_steps: Annotated[List[Tuple], operator.add]
    response: str

# Step ë³„ ì •ì˜ (1) Planning Step
from pydantic import BaseModel # pythonì—ì„œ ë°ì´í„° ê²€ì¦, êµ¬ì¡° ì •ì˜ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” í´ë˜ìŠ¤
from pydantic import Field # ê° í•„ë“œì˜ ê¸°ë³¸ê°’, ì„¤ëª…, ê²€ì¦ ì¡°ê±´ë“±ì„ ì„¤ì •í•˜ëŠ” ë„ìš°ë¯¸ í•¨ìˆ˜

class Plan(BaseModel):
    """ê³„íšì„ ì œì•ˆí•˜ëŠ” ëª¨ë¸"""
    steps: List[str] = Field(description="ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì‘ì—… ë‹¨ê³„ë“¤ì„ ë‚˜ì—´í•œ ë¦¬ìŠ¤íŠ¸, ì •ë ¬ëœ ìˆœì„œì—¬ì•¼ í•¨.")

from langchain_core.prompts import ChatPromptTemplate
planner_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """ë‹¹ì‹ ì€ ë³µì¡í•œ ì‘ì—…ì„ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ ê³„íšì„ ì„¸ì›Œì•¼ í•©ë‹ˆë‹¤.

ì¤‘ìš”í•œ ê³„íš ìˆ˜ë¦½ ì›ì¹™:
1. êµ¬ì²´ì„±: ê° ë‹¨ê³„ëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, ëª¨í˜¸í•œ í‘œí˜„ì„ í”¼í•˜ì„¸ìš”
2. ì—°ì†ì„±: ê° ë‹¨ê³„ëŠ” ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
3. ë§¥ë½ ìœ ì§€: ëª¨ë“  ë‹¨ê³„ì—ì„œ ì›ë˜ ì§ˆë¬¸ì˜ ë§¥ë½ê³¼ í‚¤ì›Œë“œë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”
4. ì‹ë³„ì ìœ ì§€: ì§ˆë¬¸ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ì‹ë³„ì(ì§€ì—­ëª…, ì‹œê°„, ì£¼ì œ, ëŒ€ìƒ ë“±)ë¥¼ ëª¨ë“  ë‹¨ê³„ì—ì„œ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”
5. ì‹¤í–‰ ê°€ëŠ¥ì„±: ê° ë‹¨ê³„ëŠ” ì£¼ì–´ì§„ ë„êµ¬ë“¤ë¡œ ì‹¤ì œë¡œ ìˆ˜í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤
6. ì™„ì„±ë„: ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤

ê³„íš ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:
- ê° ë‹¨ê³„ëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ì°¸ì¡°í•´ì•¼ í•©ë‹ˆë‹¤
- ë‹¨ê³„ ê°„ ì •ë³´ ì „ë‹¬ì´ ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤
- ì§ˆë¬¸ì˜ í•µì‹¬ ì‹ë³„ì(ì˜ˆ: íŠ¹ì • ì§€ì—­, ì‹œê°„, ì£¼ì œ, ëŒ€ìƒ ë“±)ë¥¼ ê° ë‹¨ê³„ì—ì„œ ë°˜ë“œì‹œ ìœ ì§€í•˜ì„¸ìš”
- ë„ˆë¬´ ë§ì€ ë‹¨ê³„ë¥¼ ë§Œë“¤ì§€ ë§ê³ , í•µì‹¬ì ì¸ 3-4ë‹¨ê³„ë¡œ êµ¬ì„±í•˜ì„¸ìš”
- ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ë°˜ë“œì‹œ ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ë‹¨ê³„ì—¬ì•¼ í•©ë‹ˆë‹¤

ì´ì œ ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”."""
    ),
    (
        "placeholder", # messages ë¼ëŠ” ë³€ìˆ˜ê°’ì— ì €ì¥ë˜ëŠ” ëŒ€í™” ë¦¬ìŠ¤íŠ¸ ì „ì²´ê°€ ë“¤ì–´ê°€ë„ë¡ í•˜ëŠ” ì—­í• 
        "{messages}"
    ),
])
planner = planner_prompt | llm.with_structured_output(Plan)
# result = planner.invoke({"messages": [("user", "2025ë…„ ê°€ì¥ í•«í•œ ë¶€ì‚° ê´€ê´‘ì§€ 3ê°œ ì†Œê°œí•´ì£¼ì„¸ìš”.")]})
# print(result)

# Step ë³„ ì •ì˜ (2) Re-Plan Step
from typing import Union # ì—¬ëŸ¬ íƒ€ì…ì¤‘ í•˜ë‚˜ ê°€ì§ˆìˆ˜ìˆìŒ.

class Response(BaseModel):
    """ì‚¬ìš©ìì—ê²Œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ëª¨ë¸"""
    response: str
    
class Act(BaseModel):
    """ìˆ˜í–‰í•  ì‘ì—…ì„ ì •ì˜í•˜ëŠ” ëª¨ë¸"""
    action: Union[Response, Plan] = Field(
        description="ìˆ˜í–‰í•  ì‘ì—…ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ë°”ë¡œ ì‘ë‹µí•˜ë ¤ë©´ Responseë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì •ë‹µì„ ì–»ê¸° ìœ„í•´ ë„êµ¬ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•´ì•¼ í•œë‹¤ë©´ Planì„ ì‚¬ìš©í•˜ì„¸ìš”"
    )

# ë…¸ë“œ ì •ì˜
from langgraph.graph import END

async def plan_execute_node(state: PlanExecute):
    """í˜„ì¬ ê³„íšì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    plan = state.get("plan", [])
    if not plan:
        return {"response": "ëª¨ë“  ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
    
    # í˜„ì¬ ì‹¤í–‰í•  ì‘ì—…
    current_task = plan[0]
    past_steps = state.get("past_steps", [])
    original_input = state.get("input", "")
    
    # ì´ì „ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ë¥¼ ì •ë¦¬
    context_info = ""
    if past_steps:
        for i, (task, result) in enumerate(past_steps, 1):
            context_info += f"\n{i}. {task}"
    
    # ì‘ì—… ì‹¤í–‰ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    task_formatted = f"""ì›ë˜ ì§ˆë¬¸: {original_input}

í˜„ì¬ ìˆ˜í–‰í•  ì‘ì—…: {current_task}

ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰í–ˆë˜ ì‘ì—…: {context_info}

ìœ„ì˜ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."""

    agent_response = await agent_executor.ainvoke(
        {"messages": [("user", task_formatted)]}
    )
    
    # ì™„ë£Œëœ ì‘ì—…ì„ planì—ì„œ ì œê±°í•˜ê³  past_stepsì— ì¶”ê°€
    remaining_plan = plan[1:] if len(plan) > 1 else []
    
    return {
        "past_steps": [(current_task, agent_response["messages"][-1].content)],
        "plan": remaining_plan
    }

async def plan_node(state: PlanExecute):
    """ì´ˆê¸° ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤."""
    plan = await planner.ainvoke({"messages":[("user",state["input"])]})
    return {"plan": plan.steps}

async def answer_node(state: PlanExecute):
    """ë‚¨ì•„ìˆëŠ” ê³„íšì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    plan = state.get("plan", [])
    past_steps = state.get("past_steps", [])
    original_input = state.get("input", "")
    
    # ëª¨ë“  ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë©´ LLMì„ ì‚¬ìš©í•´ ìµœì¢… ì‘ë‹µ ìƒì„±
    if not plan:
        # ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •ë¦¬
        collected_info = "\n\n".join([f"ë‹¨ê³„ {i+1}: {task}\nê²°ê³¼: {result}" 
                                     for i, (task, result) in enumerate(past_steps)])
        
        # LLMì—ê²Œ ìµœì¢… ì‘ë‹µ ìƒì„± ìš”ì²­
        final_prompt = f"""ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ ìˆ˜ì§‘ ê³¼ì •ì…ë‹ˆë‹¤:

ì‚¬ìš©ì ì§ˆë¬¸: {original_input}

ìˆ˜ì§‘ëœ ì •ë³´:
{collected_info}

ìœ„ì˜ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ê¹”ë”í•˜ê³  ìœ ìš©í•œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë…¼ë¦¬ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        final_response = await llm.ainvoke([("user", final_prompt)])
        return {"response": final_response.content}
    
    # ì•„ì§ ë‚¨ì€ ë‹¨ê³„ê°€ ìˆìœ¼ë©´ ê³„ì† ì§„í–‰
    return {"plan": plan}

def should_end(state: PlanExecute):
    if "response" in state and state["response"]:
        return END
    else:
        return "plan_execute_node"
    
# ê·¸ë˜í”„ ì •ì˜
from langgraph.graph import StateGraph, START

graph_builder = StateGraph(PlanExecute)
graph_builder.add_node("plan_node", plan_node)
graph_builder.add_node("answer_node", answer_node)
graph_builder.add_node("plan_execute_node", plan_execute_node)

graph_builder.add_edge(START, "plan_node")
graph_builder.add_edge("plan_node", "plan_execute_node")
graph_builder.add_edge("plan_execute_node", "answer_node")

graph_builder.add_conditional_edges(
    "answer_node",
    should_end,
    ["plan_execute_node", END]
)

graph = graph_builder.compile()

# ê·¸ë˜í”„ ì´ë¯¸ì§€ ì €ì¥
from utils.util import save_graph_image
save_graph_image(graph=graph,
                 filename="plan_and_execute_graph.png")

config = {"recursion_limit": 50}
inputs = {"input": "2025ë…„ ê°€ì¥ í•«í•œ ë¶€ì‚° ê´€ê´‘ì§€ 3ê°œ ì†Œê°œí•´ì£¼ì„¸ìš”."}


from rich.console import Console
from rich.panel import Panel
console = Console()
async def run_workflow():
    console.print()
    console.print(Panel(
        "[bold blue]ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹œì‘...[/bold blue]",
        border_style="blue",
        padding=(1, 2),
        width=120,
        title="[bold]LangGraph Plan & Execute[/bold]",
        title_align="center"
    ), justify="center")
    console.print()
    async for event in graph.astream(inputs, config=config):
        console.print("â”€" * 120)
        console.print()
        console.print(event)
        console.print()

# ë¹„ë™ê¸° ì‹¤í–‰
import asyncio
asyncio.run(run_workflow())